# Fluent Voice

[![Crates.io](https://img.shields.io/crates/v/fluent_voice.svg)](https://crates.io/crates/fluent_voice)
[![Documentation](https://docs.rs/fluent_voice/badge.svg)](https://docs.rs/fluent_voice)
[![License](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](LICENSE)

A pure-trait fluent builder API for Text-to-Speech (TTS) and Speech-to-Text (STT) engines in Rust.

## ğŸ¯ Design Philosophy

Fluent Voice follows a simple, elegant pattern for all voice operations:

**One fluent chain â†’ One matcher closure â†’ One `.await?`**

This design eliminates the complexity of multiple awaits, nested async calls, and scattered error handling that plague many voice APIs.

## âœ¨ Features

- **ğŸ”— Unified API**: Single interface for both TTS and STT operations
- **âš¡ Single Await**: All operations complete with exactly one `.await?`
- **ğŸ­ Multi-Speaker**: Built-in support for conversations with multiple speakers
- **ğŸ”§ Engine Agnostic**: Works with any TTS/STT engine through trait implementations
- **ğŸ›ï¸ Rich Configuration**: Comprehensive settings for voice control, audio processing, and recognition
- **ğŸ“Š Streaming**: Real-time audio streams and transcript processing
- **ğŸ›¡ï¸ Type Safe**: Leverages Rust's type system for compile-time correctness
- **ğŸ“ Well Documented**: Extensive documentation with practical examples

## ğŸ“¦ Installation

Add this to your `Cargo.toml`:

```toml
[dependencies]
fluent_voice = "0.1.0"
```

For async runtime support, also add:

```toml
tokio = { version = "1", features = ["full"] }
futures-util = "0.3"
```

## ğŸš€ Fluent API Quick Start

### Speach to text example

### Text-to-Speech Example

```rust

    // Complete fluent chain with microphone support, VAD and wake word detection
    let _transcript = FluentVoice::stt()
        .with_source(SpeechSource::Microphone {
            backend: MicBackend::Default,
            format: AudioFormat::Pcm16Khz,
            sample_rate: 16_000,
        })
        .vad_mode(VadMode::Accurate)
        .language_hint(Language("en-US"))
        .diarization(Diarization::On)
        .word_timestamps(WordTimestamps::On)
        .punctuation(Punctuation::On)
        .on_chunk(|result| match result {
            Ok(segment) => {
                println!("Real-time: {}", segment.text());
                segment
            },
            Err(e) => TranscriptionSegmentImpl::bad_chunk(e.to_string()),
        })
        .listen(|conversation| match conversation {
            Ok(conv) => conv.into_stream(),
            Err(e) => Err(e),
        });  // Creates persistent listening stream
```

```rust
use fluent_voice::prelude::*;
use futures_util::StreamExt;

#[tokio::main]
async fn main() -> Result<(), VoiceError> {
    // Note: Requires an engine implementation (see Engine Integration below)
    let mut audio_stream = FluentVoice::tts().conversation()
        .with_speaker(
            Speaker::speaker("Narrator")
                .voice_id(VoiceId::new("voice-uuid"))
                .with_speed_modifier(VocalSpeedMod(0.9))
                .speak("Hello, world!")
                .build()
        )
        .with_speaker(
            Speaker::speaker("Bob")
                .with_speed_modifier(VocalSpeedMod(1.1))
                .speak("Hi Alice! How are you today?")
                .build()
        )
        .on_chunk(|result| match result {
            Ok(synthesis_chunk) => synthesis_chunk.into(),
            Err(e) => AudioChunk::bad_chunk(e.to_string()),
        })
        .synthesize(|conversation| {
            Ok => conversation.into_stream(),  // Returns audio stream
            Err(e) => Err(e),
        })
        .await;  // Fully unwrapped stream

    // Process audio samples
    while let Some(sample) = audio_stream.next().await {
        // Play sample or save to file
        println!("Audio sample: {}", sample);
    }

    Ok(())
}
```

### Speech-to-Text Example

```rust
use fluent_voice::prelude::*;
use futures_util::StreamExt;

#[tokio::main]
async fn main() -> Result<(), VoiceError> {
    let mut transcript_stream = FluentVoice::stt(). conversation()
        .with_source(SpeechSource::Microphone {
            backend: MicBackend::Default,
            format: AudioFormat::Pcm16Khz,
            sample_rate: 16_000,
        })
        .vad_mode(VadMode::Accurate)
        .language_hint(Language("en-US"))
        .diarization(Diarization::On)  // Speaker identification
        .word_timestamps(WordTimestamps::On)
        .punctuation(Punctuation::On)
        .on_chunk(|result| match result {
            Ok(transcription_chunk) => transcription_chunk.into(),
            Err(e) => TranscriptionSegmentImpl::bad_chunk(e.to_string()),
        })
        .listen(|conversation| {
            Ok  => conversation.into_stream(),  // Returns transcript stream
            Err(e) => Err(e),
        })
        .await;  // fully unwrapped Stream<TranscriptSegment>

    // Process transcript segments
    while let Some(result) = transcript_stream.next().await {
        match result {
            Ok(segment) => {
                println!("[{:.2}s] {}: {}",
                    segment.start_ms() as f32 / 1000.0,
                    segment.speaker_id().unwrap_or("Unknown"),
                    segment.text()
                );
            },
            Err(e) => eprintln!("Recognition error: {}", e),
        }
    }

    Ok(())
}
```

## ğŸ—ï¸ Architecture

Fluent Voice is built around a pure-trait architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Code     â”‚    â”‚  Fluent Voice    â”‚    â”‚ Engine Impls    â”‚
â”‚                 â”‚    â”‚    (Traits)      â”‚    â”‚   (Concrete)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ .conversation() â”‚â”€â”€â”€â–¶â”‚ TtsConversation  â”‚â—€â”€â”€â”€â”‚ ElevenLabsImpl  â”‚
â”‚ .with_speaker() â”‚    â”‚ Builder          â”‚    â”‚ OpenAIImpl      â”‚
â”‚ .synthesize()   â”‚    â”‚                  â”‚    â”‚ AzureImpl       â”‚
â”‚ .await?         â”‚    â”‚ SttConversation  â”‚    â”‚ GoogleImpl      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ Builder          â”‚    â”‚ WhisperImpl     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Traits

- **`TtsEngine`** / **`SttEngine`**: Engine registration and initialization
- **`TtsConversationBuilder`** / **`SttConversationBuilder`**: Fluent configuration API
- **`TtsConversation`** / **`SttConversation`**: Runtime session objects
- **`Speaker`** / **`SpeakerBuilder`**: Voice and speaker configuration
- **`TranscriptSegment`** / **`TranscriptStream`**: STT result handling

## ğŸ”§ Configuration Options

### TTS Configuration

```rust
let conversation = engine.conversation()
    .with_speaker(
        Speaker::speaker("Narrator")
            .voice_id(VoiceId::new("narrator-voice"))
            .language(Language("en-US"))
            .with_speed_modifier(VocalSpeedMod(0.8))        // Slower speech
            .with_pitch_range(PitchRange::new(80.0, 200.0)) // Pitch control
            .speak("Your text here")
            .build()
    )
    .language(Language("en-US"))  // Global language setting
    .on_chunk(|result| match result {
        Ok(synthesis_chunk) => synthesis_chunk.into(),
        Err(e) => AudioChunk::bad_chunk(e.to_string()),
    })
    .synthesize(/* matcher */)
    .await?;
```

### STT Configuration

```rust
let conversation = engine.conversation()
    .with_source(SpeechSource::File {
        path: "audio.wav".to_string(),
        format: AudioFormat::Pcm16Khz,
    })
    .vad_mode(VadMode::Accurate)                           // Voice activity detection
    .noise_reduction(NoiseReduction::High)                 // Background noise filtering
    .language_hint(Language("en-US"))                      // Language optimization
    .diarization(Diarization::On)                          // Speaker identification
    .timestamps_granularity(TimestampsGranularity::Word)   // Timing precision
    .punctuation(Punctuation::On)                          // Auto-punctuation
    .on_chunk(|result| match result {
        Ok(transcription_chunk) => transcription_chunk.into(),
        Err(e) => TranscriptionSegmentImpl::bad_chunk(e.to_string()),
    })
    .listen(|conversation|{ Ok => })
    .await?;
```

## ğŸ”Œ Engine Integration

Fluent Voice is designed to work with any TTS/STT service. Engine implementations provide concrete types that implement the core traits.

### Available Engines

- **ElevenLabs**: `elevenlabs-fluent-voice` (planned)
- **OpenAI**: `openai-fluent-voice` (planned)
- **Azure Cognitive Services**: `azure-fluent-voice` (planned)
- **Google Cloud**: `google-fluent-voice` (planned)
- **Local Whisper**: `whisper-fluent-voice` (planned)

### Implementing Your Own Engine

```rust
use fluent_voice::prelude::*;

// 1. Define your engine struct
pub struct MyEngine {
    api_key: String,
}

// 2. Implement the engine trait
impl TtsEngine for MyEngine {
    type Conv = MyConversationBuilder;

    fn conversation(&self) -> Self::Conv {
        MyConversationBuilder::new(self.api_key.clone())
    }
}

// 3. Implement the conversation builder
pub struct MyConversationBuilder { /* ... */ }

impl TtsConversationBuilder for MyConversationBuilder {
    type Conversation = MyConversation;

    fn with_speaker<S: Speaker>(self, speaker: S) -> Self { /* ... */ }
    fn language(self, lang: Language) -> Self { /* ... */ }

    fn synthesize<F, R>(self, matcher: F) -> impl Future<Output = R> + Send
    where F: FnOnce(Result<Self::Conversation, VoiceError>) -> R + Send + 'static
    {
        async move {
            // Perform synthesis and call matcher with result
            let result = self.do_synthesis().await;
            matcher(result)
        }
    }
}

// 4. Implement the conversation object
impl TtsConversation for MyConversation {
    type AudioStream = impl Stream<Item = i16> + Send + Unpin;

    fn into_stream(self) -> Self::AudioStream {
        // Convert to audio stream
    }
}
```

## ğŸ“š Advanced Usage

### Error Handling with Graceful Fallbacks

```rust
let audio = primary_engine.conversation()
    .with_speaker(speaker)
    .synthesize(|conversation| {
        match conversation {
            Ok(conv) => Ok(conv.into_stream()),
            Err(primary_error) => {
                eprintln!("Primary engine failed: {}", primary_error);
                // Could try fallback engine here
                Err(primary_error)
            }
        }
    })
    .await
    .or_else(|_| {
        // Fallback to different engine or settings
        fallback_engine.conversation()
            .with_speaker(speaker)
            .on_chunk(|result| match result {
                Ok(synthesis_chunk) => synthesis_chunk.into(),
                Err(e) => AudioChunk::bad_chunk(e.to_string()),
            })
            .synthesize(|conv| {
                Ok => conv.into_stream(),
                Err(e) => Err(e),
            })
    })?;
```

### Real-time Audio Processing

```rust
let mut audio_stream = engine.conversation()
    .with_speaker(speaker)
    .on_chunk(|result| match result {
        Ok(synthesis_chunk) => synthesis_chunk.into(),
        Err(e) => AudioChunk::bad_chunk(e.to_string()),
    })
    .synthesize(|conv| Ok => conv.into_stream(), Err(e) => Err(e))
    .await?;

// Apply real-time effects
while let Some(sample) = audio_stream.next().await {
    let processed_sample = apply_effects(sample);
    audio_output.play(processed_sample)?;
}
```

### Batch Transcript Processing

```rust
let mut transcript_stream = engine.conversation()
    .with_source(SpeechSource::from_file("meeting.wav", AudioFormat::Pcm16Khz))
    .diarization(Diarization::On)
    .on_chunk(|result| match result {
        Ok(transcription_chunk) => transcription_chunk.into(),
        Err(e) => TranscriptionSegmentImpl::bad_chunk(e.to_string()),
    })
    .listen(|conv| Ok => conv.into_stream(), Err(e) => Err(e))
    .await?;

// Collect and format transcript
let mut segments = Vec::new();
while let Some(result) = transcript_stream.next().await {
    if let Ok(segment) = result {
        segments.push(segment);
    }
}

// Generate formatted output
generate_transcript_document(segments)?;
```

## ğŸ§ª Testing

Run the test suite:

```bash
cargo test
```

Run examples:

```bash
cargo run --example api_usage
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Setup

1. Clone the repository
2. Install Rust (latest stable)
3. Run tests: `cargo test`
4. Run examples: `cargo run --example api_usage`

### Code Style

- Follow Rust conventions and `cargo fmt`
- Add tests for new functionality
- Document public APIs with examples
- Use `cargo clippy` for linting

## ğŸ“„ License

This project is licensed under either of

- Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or <http://www.apache.org/licenses/LICENSE-2.0>)
- MIT license ([LICENSE-MIT](LICENSE-MIT) or <http://opensource.org/licenses/MIT>)

at your option.

## ğŸ™‹ Support

- ğŸ“– [Documentation](https://docs.rs/fluent_voice)
- ğŸ› [Issue Tracker](https://github.com/your-org/fluent_voice/issues)
- ğŸ’¬ [Discussions](https://github.com/your-org/fluent_voice/discussions)

---

Made with â¤ï¸ for the Rust community
